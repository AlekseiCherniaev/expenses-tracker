services:
  api:
    image: ${DOCKERHUB_USERNAME}/expenses_tracker:latest
    container_name: api
    networks:
      - common_network
    expose:
      - 8000
    env_file:
      - .env
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8000/api/health\")' || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  postgres:
    image: postgres:15-alpine
    container_name: postgres
    ports:
      - "${POSTGRES_PORT}:5432"
    env_file:
      - .env
    volumes:
      - postgres:/var/lib/postgresql/data
    mem_limit: 1024m
    cpus: 0.5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1024M
        reservations:
          cpus: '0.2'
          memory: 256M
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}" ]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      - common_network

  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    env_file:
      - .env
    volumes:
      - minio-data:/data
    mem_limit: 256m
    cpus: 0.2
    deploy:
      resources:
        limits:
          cpus: '0.20'
          memory: 256M
        reservations:
          cpus: '0.10'
          memory: 128M
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      - common_network

  redis:
    image: redis:7-alpine
    container_name: redis
    command: [ "redis-server", "--appendonly", "yes" ]
    volumes:
      - redis-data:/data
    env_file:
      - .env
    mem_limit: 64m
    cpus: 0.1
    deploy:
      resources:
        limits:
          cpus: '0.10'
          memory: 64M
        reservations:
          cpus: '0.05'
          memory: 32M
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      - common_network

  migrations:
    container_name: migrations
    build: .
    command: [ "/app/.venv/bin/alembic", "upgrade", "head" ]
    env_file:
      - .env
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - common_network

  nginx:
    image: ${DOCKERHUB_USERNAME}/expenses_frontend:latest
    container_name: nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - certbot-etc:/etc/letsencrypt
      - certbot-var:/var/lib/letsencrypt
      - ./certbot-www:/var/www/certbot
      - /etc/nginx/.htpasswd:/etc/nginx/.htpasswd:ro
    depends_on:
      - api
    networks:
      - common_network

  certbot:
    image: certbot/certbot
    container_name: certbot
    volumes:
      - certbot-etc:/etc/letsencrypt
      - certbot-var:/var/lib/letsencrypt
    env_file:
      - .env
    command: [
      "certonly",
      "--standalone",
      "--preferred-challenges", "http",
      "--email=${CERTBOT_EMAIL}",
      "--agree-tos",
      "--no-eff-email",
      "-d", "${DOMAIN}",
      "-d", "www.${DOMAIN}",
      "-d", "storage.${DOMAIN}",
      "-d", "grafana.${DOMAIN}",
      "-d", "prometheus.${DOMAIN}",
      "--expand"
    ]

  certbot-renew:
    build:
      context: .
      dockerfile: Dockerfile.certbot-renew
    container_name: certbot_renew
    volumes:
      - certbot-etc:/etc/letsencrypt
      - certbot-var:/var/lib/letsencrypt
      - ./certbot-www:/var/www/certbot
      - /var/run/docker.sock:/var/run/docker.sock
    entrypoint: sh -c "while :; do certbot renew --webroot -w /var/www/certbot && docker exec nginx nginx -s reload; sleep 12h; done"
    restart: unless-stopped
    networks:
      - common_network

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    expose:
      - "9090"
    networks:
      - common_network
    depends_on:
      - api

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    expose:
      - "3000"
    mem_limit: 512m
    cpus: 0.3
    networks:
      - common_network
    environment:
      - GF_SERVER_ROOT_URL=https://grafana.${DOMAIN}/
      - GF_INSTALL_PLUGINS=grafana-pyroscope-datasource
      - HTTP_PROXY=http://${GRAFANA_PROXY_USER}:${GRAFANA_PROXY_PASSWORD}@${GRAFANA_PROXY_HOST}:${GRAFANA_PROXY_PORT}
      - HTTPS_PROXY=http://${GRAFANA_PROXY_USER}:${GRAFANA_PROXY_PASSWORD}@${GRAFANA_PROXY_HOST}:${GRAFANA_PROXY_PORT}
      - NO_PROXY=localhost,127.0.0.1
    depends_on:
      - prometheus
      - loki
      - tempo
    volumes:
      - grafana-data:/var/lib/grafana
      - ./configs/grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml

  loki:
    image: grafana/loki:2.9.2
    container_name: loki
    expose:
      - "3100"
    command: -config.file=/etc/loki/local-config.yaml
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    volumes:
      - ./configs/loki.yaml:/etc/loki/local-config.yaml
      - loki-data:/loki
    networks:
      - common_network
    depends_on:
      - minio

  promtail:
    image: grafana/promtail:2.9.2
    container_name: promtail
    expose:
      - "9080"
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/log:/var/log
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./configs/promtail.yaml:/etc/promtail/config.yml
      - promtail-positions:/tmp
    networks:
      - common_network

  tempo_init:
    image: grafana/tempo:latest
    user: root
    entrypoint: ["chown"]
    command: ["-R", "10001:10001", "/var/tempo"]
    volumes:
      - tempo-wal:/var/tempo/wal
      - tempo-blocks:/var/tempo/blocks
      - tempo-generator:/var/tempo/generator
    networks:
      - common_network

  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    command: -config.file=/etc/tempo.yaml
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    volumes:
      - ./configs/tempo.yaml:/etc/tempo.yaml
      - tempo-wal:/var/tempo/wal
      - tempo-blocks:/var/tempo/blocks
      - tempo-generator:/var/tempo/generator
    ports:
      - "3200:3200"
      - "4317:4317"
      - "4318:4318"
    networks:
      - common_network
    depends_on:
      tempo_init:
          condition: service_completed_successfully
      minio:
          condition: service_started
    healthcheck:
      test: [ "CMD-SHELL", "timeout 10s bash -c ':> /dev/tcp/127.0.0.1/3200' || exit 0" ]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  common_network:
    name: common_network
    driver: bridge

volumes:
  minio-data:
  redis-data:
  postgres:
    driver: local
    name: expenses_postgres_data
  certbot-etc:
  certbot-var:
  grafana-data:
  loki-data:
  promtail-positions:
  tempo-wal:
  tempo-blocks:
  tempo-generator: